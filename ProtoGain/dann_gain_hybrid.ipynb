{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# protogain\n",
    "from model import Network\n",
    "from hypers import Params\n",
    "from dataset import generate_hint\n",
    "from output import Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DANN & GAIN hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, latent_dim: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(latent_dim)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim: int, hidden_dim: int, target_dim: int):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, target_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainClassifier(nn.Module):\n",
    "    \"\"\" Distinguish the domain of the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, n_class: int):\n",
    "        super(DomainClassifier, self).__init__()\n",
    "\n",
    "        # in the end is a logistic regressor\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.domain_classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambd=1.0):\n",
    "        ctx.lambd = lambd\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.lambd, None\n",
    "\n",
    "class GradientReversalLayer(nn.Module):\n",
    "    def __init__(self, lambd=1.0):\n",
    "        super(GradientReversalLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAIN_DANN(nn.Module):\n",
    "    def __init__(self, input_dim: int, latent_dim: int, n_class: int, params: Params, metrics: Metrics):\n",
    "        super(GAIN_DANN, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_dim=input_dim, hidden_dim=128, latent_dim=latent_dim)\n",
    "        \n",
    "        # gradient reversal layer\n",
    "        self.grl = GradientReversalLayer()\n",
    "\n",
    "        self.domain_classifier = DomainClassifier(latent_dim, n_class=n_class)\n",
    "        \n",
    "        # gain\n",
    "        self.gain = Network(hypers=params, \n",
    "                            net_G= nn.Sequential(\n",
    "                                nn.Linear(latent_dim* 2, latent_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(latent_dim, latent_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(latent_dim, latent_dim),\n",
    "                                nn.Sigmoid(),\n",
    "                            ), \n",
    "                            net_D= nn.Sequential(\n",
    "                                nn.Linear(latent_dim * 2, latent_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(latent_dim, latent_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(latent_dim, latent_dim),\n",
    "                                nn.Sigmoid(),\n",
    "                            ),\n",
    "                            metrics=metrics)\n",
    "        \n",
    "        self.decoder = Decoder(latent_dim=latent_dim, hidden_dim=128, target_dim=input_dim)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Forward pass for GAIN_DANN.\n",
    "            Handles missing values (NaNs) by replacing them with noise and using a mask.\n",
    "        \"\"\"\n",
    "\n",
    "        #todo x must be scaled\n",
    "\n",
    "        x_filled = x.clone()\n",
    "        x_filled[torch.isnan(x_filled)] = 0 # x filled with zeros in the place of missing values\n",
    "\n",
    "        mask = (~torch.isnan(x)).float()\n",
    "\n",
    "        # 1. Encode\n",
    "        x_encoded = self.encoder(x_filled)\n",
    "        x_grl = self.grl(x_encoded) # as a matter of fact, this is not needed, this layer is important for the training process\n",
    "\n",
    "        # 2. Gain\n",
    "        sample = self.gain.generate_sample(x_grl, mask)\n",
    "        x_imputed = x_encoded * mask + sample * (1 - mask)\n",
    "\n",
    "        # 2.1. Domain Classifier\n",
    "        x_domain = self.domain_classifier(x_encoded)\n",
    "        x_domain = torch.argmax(x_domain, dim=1)\n",
    "\n",
    "        # 3. Decoder\n",
    "        x_reconstructed = self.decoder(x_imputed)\n",
    "\n",
    "        #todo voltar a transformar para a escala antes de ser scaled\n",
    "\n",
    "        return x_reconstructed, x_domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, num_epochs: int, hint_rate: float):\n",
    "\n",
    "    # Losses\n",
    "    domain_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_domain = torch.optim.Adam(model.domain_classifier.parameters())\n",
    "    optimizer_encoder = torch.optim.Adam(model.encoder.parameters())\n",
    "    optimizer_decoder = torch.optim.Adam(model.decoder.parameters())\n",
    "\n",
    "    # torch.autograd.set_detect_anomaly(True) test purposes\n",
    "\n",
    "    # initialize weights encoder\n",
    "    model.encoder.apply(init_weights)\n",
    "    model.decoder.apply(init_weights)\n",
    "\n",
    "    rmse_per_epoch = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "\n",
    "        domain_accuracies = [] # across batches\n",
    "        reconstruction_errors = [] # reconstruction error\n",
    "        domain_losses = []\n",
    "        gain_losses = []\n",
    "        encoder_losses = []\n",
    "\n",
    "        for x, domain_target in train_loader:\n",
    "            x_filled = x.clone()\n",
    "            x_filled[torch.isnan(x_filled)] = 0 # x filled with zeros in the place of missing values\n",
    "\n",
    "            mask = (~torch.isnan(x)).float()\n",
    "            hint = generate_hint(mask, hint_rate)\n",
    "\n",
    "            # =============================================\n",
    "            #               Encoder training\n",
    "            # =============================================\n",
    "\n",
    "            p = float(epoch) / num_epochs # training progress 0 -> 1\n",
    "            lambda_dann = 5 * (2. / (1 + np.exp(-10 * p)) - 1) # the paper sets y equal to 10 (just empirically)\n",
    "\n",
    "            x_encoded = model.encoder(x_filled)\n",
    "            x_grl = model.grl(x_encoded)\n",
    "\n",
    "            domain_pred = model.domain_classifier(x_grl) # domain labels prediction\n",
    "            domain_loss = domain_criterion(domain_pred, domain_target)\n",
    "            \n",
    "            encoder_adv_loss = lambda_dann * domain_loss # cross entropy loss\n",
    "            encoder_losses.append(encoder_adv_loss.item())\n",
    "\n",
    "            optimizer_encoder.zero_grad()\n",
    "            encoder_adv_loss.backward(retain_graph=True)\n",
    "            optimizer_encoder.step()\n",
    "\n",
    "            # =============================================\n",
    "            #          Domain Classifier training\n",
    "            # =============================================\n",
    "\n",
    "            x_encoded_detach = x_encoded.detach()\n",
    "\n",
    "            domain_pred = model.domain_classifier(x_encoded_detach) # domain labels prediction\n",
    "            domain_loss = domain_criterion(domain_pred, domain_target)\n",
    "            domain_losses.append(domain_loss.item())\n",
    "\n",
    "\n",
    "            # domain classification accuracy (just to have a better insight)\n",
    "            domain_pred_labels = torch.argmax(domain_pred, dim=1)\n",
    "            domain_accuracy = (domain_pred_labels == domain_target).float().mean().item()\n",
    "            domain_accuracies.append(domain_accuracy)\n",
    "\n",
    "            optimizer_domain.zero_grad()\n",
    "            domain_loss.backward(retain_graph=True)\n",
    "            optimizer_domain.step()\n",
    "\n",
    "            # =============================================\n",
    "            #               GAIN training\n",
    "            # - as in function model.py/train\n",
    "            # =============================================\n",
    "\n",
    "            loss_gain = nn.BCELoss(reduction=\"none\")\n",
    "            loss_mse_gain = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "            dim = x_encoded.shape[1]\n",
    "            n_samples = x_encoded.shape[0] # number of examples/samples\n",
    "\n",
    "            Z = torch.rand((n_samples, dim)) * 0.01\n",
    "\n",
    "            x_encoded_tensor = torch.from_numpy(x_encoded.detach().numpy())\n",
    "            model.gain._update_D(x_encoded_tensor, mask, hint, Z, loss_gain)\n",
    "            model.gain._update_G(x_encoded_tensor, mask, hint, Z, loss_gain)\n",
    "\n",
    "            samples = model.gain.generate_sample(x_encoded_tensor, mask)\n",
    "\n",
    "            loss_mse = loss_mse_gain(mask * x_encoded_tensor, mask * samples)\n",
    "            loss_mse = loss_mse.detach().cpu().numpy()\n",
    "            gain_losses.append(loss_mse.mean())\n",
    "\n",
    "            x_imputed_aux = x_encoded_tensor * mask + samples * (1 - mask)\n",
    "\n",
    "            # =============================================\n",
    "            #               Decoder training\n",
    "            # =============================================\n",
    "\n",
    "            x_imputed = x_imputed_aux.clone().detach().requires_grad_(True)\n",
    "            x_reconstructed = model.decoder(x_imputed)\n",
    "\n",
    "            x_missing = x.clone().detach() # ground truth\n",
    "            x_missing[torch.isnan(x_missing)] = 0\n",
    "\n",
    "            squared_error = (x_reconstructed - x_missing) ** 2 # MSE error\n",
    "            reconstruction_loss = torch.sqrt((squared_error * mask).sum() / mask.sum()) # RMSE error\n",
    "            reconstruction_errors.append(reconstruction_loss.clone().detach().item())\n",
    "\n",
    "            optimizer_decoder.zero_grad()\n",
    "            reconstruction_loss.backward()\n",
    "            optimizer_decoder.step()\n",
    "\n",
    "        # rmse per epoch\n",
    "        rmse_per_epoch.append(np.mean(reconstruction_errors))\n",
    "\n",
    "        # === Evaluation ===\n",
    "        print(f\"GAIN Loss (MSE) {np.mean(gain_losses):.4f}\")\n",
    "        print(f\"Domain Loss: {np.mean(domain_losses):.4f} | Encoder Adv Loss: {np.mean(encoder_losses):.4f}\")\n",
    "        print(f\"Domain Accuracy: {np.mean(domain_accuracies):.4f}\")\n",
    "        print(f\"Reconstruction loss RMSE: {np.mean(reconstruction_errors):.4f}\")\n",
    "\n",
    "    return rmse_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "\n",
    "    print(\"\\nEvaluation...\")\n",
    "\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_squared_error = 0\n",
    "    total_mask_elements = 0\n",
    "\n",
    "    for x, x_domain in test_loader:\n",
    "        x_missing = x.clone()\n",
    "        mask = (~torch.isnan(x_missing))\n",
    "\n",
    "        x_pred, x_domain_pred = model(x_missing)\n",
    "\n",
    "        total_correct += (x_domain_pred == x_domain).sum().item()\n",
    "        total_samples += x_domain.size(0)\n",
    "\n",
    "        squared_error = (x_pred - x_missing) ** 2 # MSE error\n",
    "        squared_error[~mask] = 0\n",
    "        total_squared_error += squared_error.sum().item()\n",
    "        total_mask_elements += mask.sum().item()\n",
    "\n",
    "    domain_accuracy = total_correct / total_samples\n",
    "    rmse = (total_squared_error / total_mask_elements) ** 0.5\n",
    "\n",
    "    print(f\"Domain Accuracy: {domain_accuracy:.4f}\")\n",
    "    print(f\"Reconstruction error RMSE: {rmse:.4f}\")\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_missingness(data: pd.DataFrame, miss_rate: float=0.2):\n",
    "    size, dim = data.shape\n",
    "\n",
    "    # do not alter the last column, since it corresponds to the projects\n",
    "    mask = np.random.rand(size, dim - 1) > miss_rate\n",
    "\n",
    "    data_np = data.to_numpy()\n",
    "    missing_data_np = np.where(mask, data_np[:, :-1], np.nan)\n",
    "    last_column = data_np[:, -1:].copy()\n",
    "\n",
    "    full_missing_np = np.hstack((missing_data_np, last_column))\n",
    "\n",
    "    missing_data = pd.DataFrame(full_missing_np, columns=data.columns, index=data.index)\n",
    "\n",
    "    return missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hela = pd.read_csv('./hela_dann.csv', index_col=0)\n",
    "\n",
    "hela = hela.iloc[8000:, :] # if ran locally\n",
    "\n",
    "hela = hela.T\n",
    "project_data = hela[\"Project\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate artificial missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hela_missing = generate_missingness(hela, miss_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run with only a train and test split\n",
    "def create_dataloaders(data: pd.DataFrame, batch_size: int=64, test_size: float=0.2):\n",
    "    domain = data[\"Project\"]\n",
    "    projects = domain.unique()\n",
    "    project_to_number = {name: idx for idx, name in enumerate(projects)} # map each project with a number\n",
    "    domain_labels = torch.tensor(domain.map(project_to_number).to_numpy(), dtype=torch.long)\n",
    "    data = data.drop(columns=\"Project\")\n",
    "\n",
    "    data_values = data.values.astype(np.float32)\n",
    "    labels = domain_labels\n",
    "\n",
    "    range_scaler = (0, 1)\n",
    "    scaler = MinMaxScaler(feature_range=range_scaler)\n",
    "    scaled_values = scaler.fit_transform(data_values)\n",
    "    X = torch.tensor(scaled_values, dtype=torch.float32)\n",
    "    y = labels\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=labels, random_state=42)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # balance classes\n",
    "    train_labels = torch.tensor([y for _, y in train_dataset]) \n",
    "    class_samples_count = torch.bincount(train_labels)\n",
    "    weights = 1. / class_samples_count\n",
    "    sample_weights = weights[train_labels]\n",
    "\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation, namely [Stratified K-Fold](https://github.com/xbeat/Machine-Learning/blob/main/Stratified%20K-Fold%20Cross-Validation%20in%20Python.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainkfold(k: int=5, num_epochs: int=10):\n",
    "\n",
    "    input_dim = hela_missing.shape[1] - 1 # number of columns - 1 (due to the project column)\n",
    "    n_class = len(project_data.unique())\n",
    "\n",
    "    print(\"Number of samples:\", hela_missing.shape[0])\n",
    "    print(\"Number of proteins:\", hela_missing.shape[1] - 1)\n",
    "    print(\"Number of unique projects:\", n_class)\n",
    "\n",
    "    params = Params()\n",
    "    metrics = Metrics(params)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    data = hela_missing.copy()\n",
    "\n",
    "    domain = data[\"Project\"]\n",
    "    projects = domain.unique()\n",
    "    project_to_number = {name: idx for idx, name in enumerate(projects)} # map each project with a number\n",
    "    domain_labels = torch.tensor(domain.map(project_to_number).to_numpy(), dtype=torch.long)\n",
    "    data = data.drop(columns=\"Project\")\n",
    "\n",
    "    data_values = data.values.astype(np.float32)\n",
    "    labels = domain_labels\n",
    "\n",
    "    range_scaler = (0, 1)\n",
    "    scaler = MinMaxScaler(feature_range=range_scaler)\n",
    "    scaled_values = scaler.fit_transform(data_values)\n",
    "    X = torch.tensor(scaled_values, dtype=torch.float32)\n",
    "    y = labels\n",
    "\n",
    "    rmse_per_epoch = [[] for _ in range(num_epochs)] # rmse per epoch for every fold\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"\\n====== Fold {fold} ======\")\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "        # balance class distribution\n",
    "        train_labels = torch.tensor([y for _, y in train_dataset]) \n",
    "        class_samples_count = torch.bincount(train_labels)\n",
    "        weights = 1. / class_samples_count\n",
    "        sample_weights = weights[train_labels]\n",
    "\n",
    "        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "        model = GAIN_DANN(input_dim, latent_dim=input_dim, n_class=n_class, params=params, metrics=metrics)\n",
    "        \n",
    "        rmse = train(model, train_loader, num_epochs=num_epochs, hint_rate=0.9)\n",
    "\n",
    "        for i, val in enumerate(rmse):\n",
    "            rmse_per_epoch[i].append(val)\n",
    "\n",
    "        evaluate(model, test_loader)\n",
    "\n",
    "    return rmse_per_epoch\n",
    "\n",
    "rmse_per_epoch = mainkfold(k=5, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./rmse.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Epoch', 'Reconstruction Error'])\n",
    "    for epoch in range(len(rmse_per_epoch)):\n",
    "        for rmse in rmse_per_epoch[epoch]:\n",
    "            writer.writerow([epoch+1, rmse])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot rmse from cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_plot = []\n",
    "\n",
    "for epoch in range(len(rmse_per_epoch)):\n",
    "    rmse_plot.append(np.mean(rmse_per_epoch[epoch]))\n",
    "\n",
    "epochs = list(range(1, len(rmse_plot) + 1))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, rmse_plot, marker=\"o\", color=\"y\")\n",
    "plt.title(\"RMSE over 5 epochs (5-Fold Cross-Validation)\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xticks(epochs)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.ylim(min(rmse_plot) - 0.02, max(rmse_plot) + 0.02)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "for x, y in zip(epochs, rmse_plot):\n",
    "    if x == 1:\n",
    "        plt.text(x, y + 0.003, f\"{y:.3f}\", ha='center')\n",
    "    elif x == 2:\n",
    "        plt.text(x, y + 0.003, f\"{y:.3f}\", ha='left')\n",
    "    else:\n",
    "        plt.text(x, y + 0.005, f\"{y:.3f}\", ha='center')\n",
    "\n",
    "plt.savefig(\"./imgs/rmse_dann_gain.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".proto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
